{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782fe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/13 14:29:23 WARN Utils: Your hostname, MacBook-Pro-M5.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.232 instead (on interface en0)\n",
      "25/11/13 14:29:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/13 14:29:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了 1410 個 .mp3 檔案\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|local_path                                                  |\n",
      "+------------------------------------------------------------+\n",
      "|/Users/chenghungyeh/data/dataset-acm-mirum/2169604.clip.mp3 |\n",
      "|/Users/chenghungyeh/data/dataset-acm-mirum/6930048.clip.mp3 |\n",
      "|/Users/chenghungyeh/data/dataset-acm-mirum/11812770.clip.mp3|\n",
      "|/Users/chenghungyeh/data/dataset-acm-mirum/3048624.clip.mp3 |\n",
      "|/Users/chenghungyeh/data/dataset-acm-mirum/2204435.clip.mp3 |\n",
      "+------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. 啟動 Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Local-Audio-Pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. 找到你所有的 .mp3 檔案\n",
    "# 使用 os.path.expanduser 來處理 '~'\n",
    "data_directory = os.path.expanduser(\"~/data/dataset-acm-mirum\")\n",
    "local_files = glob.glob(os.path.join(data_directory, \"*.mp3\"))\n",
    "\n",
    "if not local_files:\n",
    "    print(f\"在 {data_directory} 中找不到 *.mp3 檔案\")\n",
    "else:\n",
    "    print(f\"找到了 {len(local_files)} 個 .mp3 檔案\")\n",
    "\n",
    "    # 3. 將 Python 列表轉換為 Spark DataFrame\n",
    "    # 為了讓 Spark 知道這是一個檔案路徑，使用 \"file://\" 協定\n",
    "    # struct 'StructField' 允許我們定義欄位名稱\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "    \n",
    "    # 將 ['path1.mp3', 'path2.mp3'] 轉換為 [('path1.mp3',), ('path2.mp3',)]\n",
    "    path_data = [(f,) for f in local_files] \n",
    "    path_schema = StructType([StructField(\"local_path\", StringType(), False)])\n",
    "\n",
    "    df = spark.createDataFrame(path_data, schema=path_schema)\n",
    "\n",
    "    df.show(5, truncate=False)\n",
    "    # +-----------------------------------------------+\n",
    "    # | local_path                                    |\n",
    "    # +-----------------------------------------------+\n",
    "    # | /Users/your_user/data/dataset-acm-mirum/1.mp3 |\n",
    "    # | /Users/your_user/data/dataset-acm-mirum/2.mp3 |\n",
    "    # ...\n",
    "    # +-----------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcc21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元資料提取完成:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------+-----+\n",
      "|                path| duration|sample_rate|error|\n",
      "+--------------------+---------+-----------+-----+\n",
      "|/Users/chenghungy...|     30.0|      22050| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...| 30.02882|      44100| NULL|\n",
      "|/Users/chenghungy...|29.947392|      22050| NULL|\n",
      "|/Users/chenghungy...|29.988209|      22050| NULL|\n",
      "|/Users/chenghungy...| 59.94739|      22050| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|30.040817|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...| 59.94739|      22050| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|29.947392|      22050| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...| 30.02882|      44100| NULL|\n",
      "|/Users/chenghungy...| 30.02882|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "|/Users/chenghungy...|     30.0|      44100| NULL|\n",
      "+--------------------+---------+-----------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType\n",
    "\n",
    "# 1. 這是我們的 UDF 函式。它非常乾淨。\n",
    "def process_local_audio(local_path):\n",
    "    try:\n",
    "        # Librosa 直接處理本地路徑\n",
    "        y, sr = librosa.load(local_path, sr=None) # sr=None 讀取原始採樣率\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        return (local_path, float(duration), int(sr), None) # (路徑, 時長, 採樣率, 錯誤)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (local_path, None, None, str(e))\n",
    "\n",
    "# 2. 定義 UDF 的輸出 Schema\n",
    "output_schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"sample_rate\", IntegerType(), True),\n",
    "    StructField(\"error\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 3. 註冊 UDF\n",
    "extract_metadata_udf = udf(process_local_audio, output_schema)\n",
    "\n",
    "# 4. 應用 UDF\n",
    "#    'df' 是我們在 Day 1/2 建立的 DataFrame\n",
    "metadata_df = df.withColumn(\"metadata\", extract_metadata_udf(col(\"local_path\"))) \\\n",
    "                .select(\"metadata.*\") # 展平 struct\n",
    "\n",
    "print(\"元資料提取完成:\")\n",
    "metadata_df.show()\n",
    "\n",
    "# +--------------------+--------+-------------+-----+\n",
    "# | path               | duration | sample_rate | error|\n",
    "# +--------------------+--------+-------------+-----+\n",
    "# | /.../1.mp3         | 30.12    | 44100       | null |\n",
    "# | /.../2.mp3         | 15.5     | 44100       | null |\n",
    "# | /.../broken.mp3    | null     | null        | ...  |\n",
    "# +--------------------+--------+-------------+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157dab3",
   "metadata": {},
   "source": [
    "步驟 2：在 Spark DataFrame 上產生統計資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f75ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_data_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfdv\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# final_df 是 Day 3 產出的 Spark DataFrame\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m正在 TFDV... (可能需要幾分鐘)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_data_validation'"
     ]
    }
   ],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from pyspark.sql.functions import col # 確保 col 被 import\n",
    "\n",
    "# 這是 Day 3 (Cell 2) 產生的 metadata_df\n",
    "print(\"正在 TFDV... (可能需要幾分鐘)\")\n",
    "\n",
    "# TFDV 可以直接在 Spark DataFrame 上運作\n",
    "# 丟掉有 null 的行，避免 TFDV 報錯\n",
    "stats = tfdv.generate_statistics_from_spark_dataframe(metadata_df.na.drop()) \n",
    "print(\"統計資料產生完畢。\")\n",
    "\n",
    "# 推斷 Schema\n",
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "\n",
    "# 在 Jupyter Notebook 中顯示互動式 UI\n",
    "# (注意: 你可能需要安裝 ipywidgets 才能看到 UI，如 Cell 5 的警告所示)\n",
    "tfdv.display_schema(schema)\n",
    "\n",
    "# --- 接著是過濾 (Filter) ---\n",
    "print(\"TFDV Schema 已顯示。現在根據規則過濾：\")\n",
    "\n",
    "# 根據 Cell 2 的輸出，我們看到 sample_rate 有 22050 和 44100\n",
    "# 假設我們的 ML 模型只接受 44100\n",
    "clean_df = metadata_df.filter(\n",
    "    (col(\"duration\") > 1.0) &  # 假設我們要 > 1 秒\n",
    "    (col(\"sample_rate\") == 44100) & # 假設我們只要 44.1k\n",
    "    (col(\"error\").isNull())\n",
    ")\n",
    "\n",
    "print(\"過濾前的總數:\", metadata_df.count())\n",
    "print(\"過濾後的乾淨資料:\", clean_df.count())\n",
    "clean_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f6598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 14:39:51,714\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在提交 1410 個 Ray tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_process_local_audio pid=4938)\u001b[0m [src/libmpg123/layer3.c:INT123_do_layer3():1804] error: dequantization failed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray 處理完成！總耗時: 22.34 秒\n",
      "\n",
      "--- Ray 處理結果 (前 5 筆) ---\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/2169604.clip.mp3', 30.0, 22050, None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/6930048.clip.mp3', 30.0, 44100, None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/11812770.clip.mp3', 30.0, 44100, None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/3048624.clip.mp3', 30.028820861678003, 44100, None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/2204435.clip.mp3', 29.947392290249432, 22050, None)\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import librosa\n",
    "import time\n",
    "import os # 確保 os 被 import\n",
    "\n",
    "# 1. (重新) 確保 Ray 已初始化\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# 2. 這是 Day 3 (Cell 2) 的函式，現在加上 @ray.remote\n",
    "@ray.remote\n",
    "def ray_process_local_audio(local_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(local_path, sr=None) \n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        return (local_path, float(duration), int(sr), None)\n",
    "    except Exception as e:\n",
    "        return (local_path, None, None, str(e))\n",
    "\n",
    "# 3. 'local_files' 列表來自 Cell 1\n",
    "#    我們在這裡重新載入它，以確保 cell 可以獨立執行\n",
    "if 'local_files' not in globals():\n",
    "    print(\"重新載入 'local_files' 列表...\")\n",
    "    data_directory = os.path.expanduser(\"~/data/dataset-acm-mirum\")\n",
    "    local_files = glob.glob(os.path.join(data_directory, \"*.mp3\"))\n",
    "\n",
    "if not local_files:\n",
    "    print(\"錯誤: 'local_files' 列表為空。\")\n",
    "else:\n",
    "    print(f\"正在提交 {len(local_files)} 個 Ray tasks...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 提交所有任務\n",
    "    futures = [ray_process_local_audio.remote(p) for p in local_files]\n",
    "    \n",
    "    # 等待並取得所有結果\n",
    "    results = ray.get(futures)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # 在 1410 個檔案上，這應該比 Spark UDF 快非常多\n",
    "    print(f\"Ray 處理完成！總耗時: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    # 4. 顯示前 5 個結果\n",
    "    print(\"\\n--- Ray 處理結果 (前 5 筆) ---\")\n",
    "    for res in results[:5]:\n",
    "        print(res)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0956e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: sympy in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Installing collected packages: safetensors, hf-xet, huggingface-hub, tokenizers, accelerate, transformers\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.31.4\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.31.4:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.31.4\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e8bf7",
   "metadata": {},
   "source": [
    "這是完整的 Day 6 程式碼，用於執行你的 Spark 標註管線："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e4220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用來自 Cell 1 的現有 Spark Session。\n",
      "開始執行 Spark + Whisper 標註管線...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：這將在 1410 個檔案上執行 Whisper。\n",
      "這會非常非常慢，因為 Spark 會為 *每個任務* 載入一次模型。\n",
      "強烈建議先用 .limit(10) 進行測試。\n",
      "--- 正在執行 5 個檔案的測試 ---\n",
      "標註處理完成。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/13 15:05:31 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 109)]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/11/13 15:05:31 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 7.0 in stage 28.0 (TID 116) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 8.0 in stage 28.0 (TID 117) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 6.0 in stage 28.0 (TID 115) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 1.0 in stage 28.0 (TID 110) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 9.0 in stage 28.0 (TID 118) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 3.0 in stage 28.0 (TID 112) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 4.0 in stage 28.0 (TID 113) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 5.0 in stage 28.0 (TID 114) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/11/13 15:05:31 WARN TaskSetManager: Lost task 2.0 in stage 28.0 (TID 111) (192.168.1.232 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 109) (192.168.1.232 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n",
      "  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n",
      "    raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:107)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:90)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n    raise ModuleNotFoundError(\nModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# 5. 過濾與顯示\u001b[39;00m\n\u001b[32m     91\u001b[39m final_clean_df = final_labeled_df.filter(col(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m).isNull())\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mfinal_clean_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# 6. (可選) 儲存到 Parquet\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m正在儲存到本地 Parquet 檔案...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/pg/lfngft2164d29lfxxj3c_c840000gn/T/ipykernel_1810/1159670147.py\", line 15, in process_and_label_local_audio\n  File \"/Users/chenghungyeh/anaconda3/envs/py311/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 2320, in __getattr__\n    raise ModuleNotFoundError(\nModuleNotFoundError: Could not import module 'pipeline'. Are this object's requirements defined correctly?\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType\n",
    "\n",
    "# 1. 這是 Day 6 (Cell 6) 的函式\n",
    "def process_and_label_local_audio(local_path):\n",
    "    # 這些 import 必須在 UDF 內部\n",
    "    import librosa\n",
    "    import warnings\n",
    "    from transformers import pipeline\n",
    "    import torch\n",
    "\n",
    "    # 1. 初始化 Whisper\n",
    "    try:\n",
    "        # 為了加速，使用 tiny.en\n",
    "        transcriber = pipeline(\n",
    "            \"automatic-speech-recognition\", \n",
    "            model=\"openai/whisper-tiny.en\",\n",
    "            device=0 if torch.cuda.is_available() else -1 # 自動使用 GPU\n",
    "        )\n",
    "    except Exception as e:\n",
    "        transcriber = None\n",
    "\n",
    "    # 2. 處理元資料\n",
    "    try:\n",
    "        y, sr = librosa.load(local_path, sr=None)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "        # 3. 處理標註 (Whisper)\n",
    "        transcription = None\n",
    "        if transcriber:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                # Whisper 可以直接讀取本地檔案路徑\n",
    "                result = transcriber(local_path)\n",
    "                transcription = result['text'] if result else None\n",
    "        else:\n",
    "            transcription = \"Transcriber failed to load\"\n",
    "\n",
    "        return (local_path, float(duration), int(sr), transcription, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        return (local_path, None, None, None, str(e))\n",
    "\n",
    "# 2. 定義 *新* 的 UDF Schema (包含 transcription)\n",
    "label_schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"sample_rate\", IntegerType(), True),\n",
    "    StructField(\"transcription\", StringType(), True), # 新欄位\n",
    "    StructField(\"error\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 3. 註冊 UDF\n",
    "label_udf = udf(process_and_label_local_audio, label_schema)\n",
    "\n",
    "# 4. 應用 UDF\n",
    "#    'df' 是來自 Cell 1 的原始 DataFrame\n",
    "#    'spark' 是來自 Cell 1 的 SparkSession\n",
    "if 'spark' not in globals() or spark.sparkContext._jsc.sc().isStopped():\n",
    "    print(\"Spark Session 已停止，正在重啟...\")\n",
    "    spark = SparkSession.builder.appName(\"Local-Audio-Pipeline-Day6\").master(\"local[*]\").getOrCreate()\n",
    "else:\n",
    "    print(\"使用來自 Cell 1 的現有 Spark Session。\")\n",
    "\n",
    "print(\"開始執行 Spark + Whisper 標註管線...\")\n",
    "print(f\"警告：這將在 {df.count()} 個檔案上執行 Whisper。\")\n",
    "print(\"這會非常非常慢，因為 Spark 會為 *每個任務* 載入一次模型。\")\n",
    "print(\"強烈建議先用 .limit(10) 進行測試。\")\n",
    "\n",
    "# --- 測試 (建議先取消註解這部分) ---\n",
    "print(\"--- 正在執行 5 個檔案的測試 ---\")\n",
    "test_df = df.limit(5)\n",
    "final_labeled_df = test_df.withColumn(\"data\", label_udf(col(\"local_path\"))) \\\n",
    "                          .select(\"data.*\")\n",
    "\n",
    "# --- 完整執行 (如果測試成功，請註解掉上面的測試) ---\n",
    "# print(\"--- 正在執行完整管線 ---\")\n",
    "# final_labeled_df = df.withColumn(\"data\", label_udf(col(\"local_path\"))) \\\n",
    "#                      .select(\"data.*\")\n",
    "\n",
    "\n",
    "print(\"標註處理完成。\")\n",
    "\n",
    "# 5. 過濾與顯示\n",
    "final_clean_df = final_labeled_df.filter(col(\"error\").isNull())\n",
    "final_clean_df.show()\n",
    "\n",
    "# 6. (可選) 儲存到 Parquet\n",
    "print(\"正在儲存到本地 Parquet 檔案...\")\n",
    "output_parquet_path = os.path.expanduser(\"~/data/dataset-acm-mirum/labeled_output.parquet\")\n",
    "final_clean_df.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
    "print(f\"資料已儲存到: {output_parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8635321",
   "metadata": {},
   "source": [
    "使用 Ray Actor（Day 5 的進階版）來更高效地執行這個標註任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c8f1066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 15:09:10,676\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在執行 5 個檔案的測試 ---\n",
      "找到了 5 個 .mp3 檔案。\n",
      "正在啟動 5 個 WhisperActor... (這會載入 5 次模型)\n",
      "Actor Pool 建立完畢 (耗時: 0.01 s)\n",
      "開始提交 5 個檔案到 Actor Pool...\n",
      "\u001b[36m(WhisperActor pid=21500)\u001b[0m Actor (PID: 21500) 正在載入模型: openai/whisper-tiny.en...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(WhisperActor pid=21500)\u001b[0m Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(WhisperActor pid=21500)\u001b[0m Actor (PID: 21500) 模型載入完畢。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(WhisperActor pid=21500)\u001b[0m Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "\u001b[36m(WhisperActor pid=21496)\u001b[0m `return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
      "\u001b[36m(WhisperActor pid=21496)\u001b[0m Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "\u001b[36m(WhisperActor pid=21496)\u001b[0m Device set to use mps:0\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(WhisperActor pid=21495)\u001b[0m Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ray Actor 處理完畢！---\n",
      "總耗時: 51.36 秒\n",
      "平均每個檔案: 10.2719 秒\n",
      "\n",
      "--- 處理結果 (前 5 筆) ---\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/11812770.clip.mp3', 30.0, 44100, \" Yeah, but then I let you go And now it's only better that I should let you know What you should know I can't live When the limit is without you\", None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/2169604.clip.mp3', 30.0, 22050, \" Love to be with you, if only I could. She wrecked the car and she was sad and so afraid that I'd be mad, but what the heck? Though I pretended hard to be, guess you could say she saw through me and hugged my neck.........................................\", None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/6930048.clip.mp3', 30.0, 44100, \" So what if it hurts me, so what if I break down, so what if this world just goes beyond the edge, my feet run out of ground, I'll have fun my place, I wanna hear myself, don't care about all the pain in front of me. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much.\", None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/2204435.clip.mp3', 29.947392290249432, 22050, \" You can take the heat, and will your heart grow cold. They say actin' just pretending. But even that gets so, and there's never any right. And you want it, a holiday game, and you want it. You can borrow the real, but you don't know. Don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't, don't\", None)\n",
      "('/Users/chenghungyeh/data/dataset-acm-mirum/3048624.clip.mp3', 30.028820861678003, 44100, \" All on your sons, oh no we gonna ride down to He like trick I value, and then we take it higher All we gonna ride down to He like trick I value, and then we take it higher He's working so hard like a soldier Can't afford a thing on TV Deep in my heart, I am warrior Thank you so much for watching, and I'll see you in the next video, and I'll see you in the next video, and I'll see you in the next video, and I'll see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video, and see you in the next video\", None)\n",
      "\u001b[36m(WhisperActor pid=21497)\u001b[0m Actor (PID: 21497) 正在載入模型: openai/whisper-tiny.en...\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(WhisperActor pid=21497)\u001b[0m Actor (PID: 21497) 模型載入完畢。\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import librosa\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from ray.util.actor_pool import ActorPool\n",
    "\n",
    "# 1. 定義 Ray Actor (這是一個有狀態的 '類別')\n",
    "@ray.remote(num_cpus=1) # 每個 Actor 限制使用 1 個 CPU 核心\n",
    "class WhisperActor:\n",
    "    \n",
    "    # __init__ (初始化) 只會在 Actor 建立時執行一次\n",
    "    def __init__(self, model_size=\"openai/whisper-tiny.en\"):\n",
    "        print(f\"Actor (PID: {os.getpid()}) 正在載入模型: {model_size}...\")\n",
    "        \n",
    "        # 決定是使用 CPU 還是 GPU\n",
    "        # device = 0 if torch.cuda.is_available() else -1 \n",
    "        # 為了穩定並行，有時在 CPU-only 的 Actor 上 'device=None' (預設 CPU) 更好\n",
    "        self.transcriber = pipeline(\n",
    "            \"automatic-speech-recognition\", \n",
    "            model=model_size,\n",
    "            device=None, # 讓 Actor 在 CPU 上運行\n",
    "            return_timestamps=True   # 這行是關鍵\n",
    "        )\n",
    "        print(f\"Actor (PID: {os.getpid()}) 模型載入完畢。\")\n",
    "\n",
    "    # 這是 Actor 的主要工作函式\n",
    "    # 它可以被遠端呼叫 (actor.transcribe.remote())\n",
    "    def transcribe_audio(self, local_path):\n",
    "        # 函式內部的 import 保持良好習慣\n",
    "        import librosa\n",
    "        import warnings\n",
    "        \n",
    "        try:\n",
    "            # --- 執行 Day 3 + Day 6 的所有工作 ---\n",
    "            \n",
    "            # 1. 處理元資料 (Day 3 邏輯)\n",
    "            y, sr = librosa.load(local_path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            \n",
    "            # 2. 處理標註 (Day 6 邏輯)\n",
    "            transcription = None\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                result = self.transcriber(local_path)\n",
    "                transcription = result['text'] if result else None\n",
    "\n",
    "            # 3. 返回完整結果\n",
    "            return (local_path, float(duration), int(sr), transcription, None)\n",
    "\n",
    "        except Exception as e:\n",
    "            return (local_path, None, None, None, str(e))\n",
    "\n",
    "# --- Main 執行緒 ---\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# 1. 載入 Cell 1 的檔案列表\n",
    "if 'local_files' not in globals():\n",
    "    print(\"重新載入 'local_files' 列表...\")\n",
    "    data_directory = os.path.expanduser(\"~/data/dataset-acm-mirum\")\n",
    "    local_files = glob.glob(os.path.join(data_directory, \"*.mp3\"))\n",
    "\n",
    "\n",
    "\n",
    "if not local_files:\n",
    "    print(\"錯誤: 'local_files' 列表為空。\")\n",
    "else:\n",
    "    # --- 測試 (建議先取消註解這部分) ---\n",
    "    print(\"--- 正在執行 5 個檔案的測試 ---\")\n",
    "    local_files = local_files[:5]\n",
    "\n",
    "    print(f\"找到了 {len(local_files)} 個 .mp3 檔案。\")\n",
    "\n",
    "    # 2. 決定 Actor 數量\n",
    "    #    使用 CPU 核心的一半，避免過度飽和 (例如 8 核心就用 4 個)\n",
    "    num_actors = max(1, os.cpu_count() // 2)\n",
    "    print(f\"正在啟動 {num_actors} 個 WhisperActor... (這會載入 {num_actors} 次模型)\")\n",
    "\n",
    "    # 3. 建立 Actor\n",
    "    #    這一步會觸發每個 Actor 的 __init__，開始載入模型\n",
    "    start_load = time.time()\n",
    "    actor_pool_list = [WhisperActor.remote() for _ in range(num_actors)]\n",
    "    pool = ActorPool(actor_pool_list)\n",
    "    print(f\"Actor Pool 建立完畢 (耗時: {time.time() - start_load:.2f} s)\")\n",
    "\n",
    "    # 4. 提交工作 (使用 Actor Pool)\n",
    "    #    pool.map_unordered 會自動將 1410 個檔案分配給可用的 Actor\n",
    "    #    這會返回一個 *generator*，它會 *即時* 產生結果\n",
    "    print(f\"開始提交 {len(local_files)} 個檔案到 Actor Pool...\")\n",
    "    start_process = time.time()\n",
    "    \n",
    "    results_generator = pool.map_unordered(\n",
    "        lambda actor, path: actor.transcribe_audio.remote(path), \n",
    "        local_files\n",
    "    )\n",
    "\n",
    "    # 5. 收集結果\n",
    "    results = []\n",
    "    processed_count = 0\n",
    "    for result in results_generator:\n",
    "        results.append(result)\n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0: # 每處理 100 個檔案就回報一次\n",
    "            print(f\"  已處理 {processed_count} / {len(local_files)} 個檔案...\")\n",
    "\n",
    "    end_process = time.time()\n",
    "    print(f\"--- Ray Actor 處理完畢！---\")\n",
    "    print(f\"總耗時: {end_process - start_process:.2f} 秒\")\n",
    "    print(f\"平均每個檔案: {(end_process - start_process) / len(local_files):.4f} 秒\")\n",
    "    \n",
    "    # 6. 顯示前 5 個結果\n",
    "    print(\"\\n--- 處理結果 (前 5 筆) ---\")\n",
    "    for res in results[:5]:\n",
    "        print(res)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f84f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
